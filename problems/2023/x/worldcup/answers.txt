Times:

10 simulations: 0m0.030s
100 simulations: 0m0.032s
1000 simulations: 0m0.039s
10000 simulations: 0m0.130s
100000 simulations: 0m1.086s
1000000 simulations: 0m9.635s

Questions:

Which predictions, if any, proved incorrect as you increased the number of simulations?:

That the runtimes would show a linear growth pattern.

Suppose you're charged a fee for each second of compute time your program uses.
After how many simulations would you call the predictions "good enough"?: TODO

It seems like 10 simulations would be considered as "good enough" assuming a fee of $1 per second of compute time and the treshold of $0.10 per acceptable simulation
